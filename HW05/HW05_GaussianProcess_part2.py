import numpy as npfrom scipy.optimize import minimizefrom HW05_GaussianProcess_util import *#objective function refer to PPT p.52def objective(trainX,trainY,beta):    def negative_log_likelihood(theta): #theta represents the kernel's hyperparameters, here the hyperparametersare are alpha and length_scale        C_theta=kernel(trainX,trainX,theta[0],theta[1]) + 1/beta * np.identity(len(trainX))        return ((1/2)*np.log(np.linalg.det(C_theta)) + (1/2)*trainY.reshape(1,-1)@np.linalg.inv(C_theta)@trainY.reshape(-1,1) + (1/2)*len(trainX)*np.log(2*np.pi)).item()    return negative_log_likelihood #return a function , refer to https://codertw.com/程式語言/359722/if __name__=='__main__':    #load file and initialize parameters    trainX,trainY=load_file()    beta=5            #maximize log likelihood is same as minimize negative log likelihood    opt_min_log_likelihood_value = 1e10    init_thetas=[10**x for x in range(-10,10)] #search for the optimal parameter by setting the initial value pairs from 1e-10 to 1e+10    for init_theta1 in init_thetas:        for init_theta2 in init_thetas:            cur_min_log_likelihood=minimize(objective(trainX,trainY,beta),x0=[init_theta1,init_theta2],bounds=((1e-5,1e5),(1e-5,1e5))) #bound to avoid dividing by 0 in kernel function            if cur_min_log_likelihood.fun < opt_min_log_likelihood_value:                opt_min_log_likelihood_value = cur_min_log_likelihood.fun                opt_theta1 , opt_theta2 = cur_min_log_likelihood.x                print(opt_theta1 , opt_theta2)    print("optimal alpha:{}, optimal length scale:{}".format(opt_theta1,opt_theta2))    #prediction refer to PPT p.48, now with the optimal parameter theta => optimal alpha and optimal length scale, the others are same as HW05_GaussianProcess_part1    C = kernel(trainX,trainX,opt_theta1,opt_theta2) + 1/beta * np.identity(len(trainX)) #we can compute the similarity between random variables s by covariance because the covariance is actually the kernel, refer to PPT p.45     testX=np.linspace(-60,60,num=100) #create test data in [-60,60]    testY_mean = kernel(trainX,testX,opt_theta1,opt_theta2).T @ np.linalg.inv(C) @ trainY.reshape(-1,1) #use gaussian regression to predict the mean of test y    testY_mean = testY_mean.reshape(-1) #for plot readibly    testY_variance = kernel(testX,testX,opt_theta1,opt_theta2)+1/beta*np.identity(len(testX)) - kernel(trainX,testX,opt_theta1,opt_theta2).T @ np.linalg.inv(C) @ kernel(trainX,testX,opt_theta1,opt_theta2) #use gaussian regression to predict the variance of test y    testY_sd = np.sqrt(np.diag(testY_variance)) #only need variance(x1,x1), not variance(x1,x2), and we convert it from variance to standard deviation        #visualization    drawing(trainX,trainY,testX,testY_mean,testY_sd)        #result    #optimal alpha:99999.99999149832, optimal length scale:2.967833657122355