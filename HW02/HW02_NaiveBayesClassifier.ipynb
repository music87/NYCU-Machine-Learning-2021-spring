{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import pdb #pdb.set_trace() #delete this line for the final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Readfile():\n",
    "    #read binary file\n",
    "    finTrainImg = open('./dataset/train-images-idx3-ubyte',mode='rb')\n",
    "    finTrainLb = open('./dataset/train-labels-idx1-ubyte',mode='rb')\n",
    "    finTestImg = open('./dataset/t10k-images-idx3-ubyte',mode='rb')\n",
    "    finTestLb = open('./dataset/t10k-labels-idx1-ubyte',mode='rb')\n",
    "\n",
    "    #deal with training headers\n",
    "    finTrainImg.seek(4,0) #first 4 bytes are the header of magic number = 0x00000803 which means the following data type is unsigned byte(meaning of 0x08) and there will be three dimensions(meaning of 0x03); to skip this header, we initially seek 4 bytes from the start of the train image file\n",
    "    finTrainLb.seek(8,0) #first 4 bytes are the header of magic number. second 4 bytes are the header of first dimension, number of images, which will be defined later; to skip these headers, we initially seek 8 bytes from the start of the train lable file\n",
    "    nTrainImg=int.from_bytes(finTrainImg.read(4), byteorder='big') #second 4 bytes are the header of first dimension, number of images = 0x0000ea60 = 0d60000\n",
    "    #nTrainImg=100 #delete\n",
    "    #finTrainImg.seek(4,1) #delete\n",
    "    nRow=int.from_bytes(finTrainImg.read(4), byteorder='big') #third 4 bytes are the header of second dimension, number of rows = 0x0000001c = 0d28\n",
    "    nCol=int.from_bytes(finTrainImg.read(4), byteorder='big') #forth 4 bytes are the header of third dimension, number of columns = 0x0000001c = 0d28\n",
    "    \n",
    "    #deal with training data and parse them\n",
    "    trainImg=np.zeros((nTrainImg,nRow,nCol),dtype='uint8') #there are 60000 images; each image has 28*28 pixels; each pixel's size is 1 byte, though unit8's memory size is larger than 1 byte ... it's python's drawback\n",
    "    trainLb=np.zeros(nTrainImg,dtype='uint8') #there are 60000 images, so there are also 60000 labels\n",
    "    for i in tqdm(range(nTrainImg)):\n",
    "        for j in range(nCol):\n",
    "            for k in range(nRow):\n",
    "                trainImg[i][j][k]=int.from_bytes(finTrainImg.read(1), byteorder='big') #actually, byteorder here can be either big or liitle, because there is just \"1\" byte    \n",
    "        trainLb[i]=int.from_bytes(finTrainLb.read(1), byteorder='big')\n",
    "    finTrainImg.close()\n",
    "    finTrainLb.close()\n",
    "\n",
    "    #same logic, deal with testing headers and data\n",
    "    finTestImg.seek(4,0)\n",
    "    finTestLb.seek(8,0)\n",
    "    nTestImg=int.from_bytes(finTestImg.read(4), byteorder='big')  #second 4 bytes are the header of first dimension, number of images = 0x00002710 = 0d10000\n",
    "    #nTestImg=100 #delete\n",
    "    #finTestImg.seek(4,1) #delete\n",
    "    finTestImg.seek(8,1) #nRow and nCol are the same as Training data\n",
    "    testImg=np.zeros((nTestImg,nRow,nCol),dtype='uint8') #there are 60000 images; each image has 28*28 pixels; each pixel's size is 1 byte, though unit8's memory size is larger than 1 byte ... it's python's drawback\n",
    "    testLb=np.zeros(nTestImg,dtype='uint8') #there are 60000 images, so there are also 60000 labels\n",
    "    for i in tqdm(range(nTestImg)):\n",
    "        for j in range(nCol):\n",
    "            for k in range(nRow):\n",
    "                testImg[i][j][k]=int.from_bytes(finTestImg.read(1), byteorder='big') #actually, byteorder here can be either big or liitle, because there is just \"1\" byte    \n",
    "        testLb[i]=int.from_bytes(finTestLb.read(1), byteorder='big')\n",
    "    finTestImg.close()\n",
    "    finTestLb.close()\n",
    "    \n",
    "    return ((trainImg,trainLb),(testImg,testLb),nRow,nCol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier ####\n",
    "\n",
    "**we want to find the most potential lable given a testImage**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "label^* &= argmax_{label}P(label|testImage)\\because \\text{bayes theorem} \\\\\n",
    "&= argmax_{label}\\frac{P(testImage|label)*P(label)}{P(testImage)} \\because P(testImage)\\text{ is irrelevant to label}\\\\\n",
    "&=argmax_{label}P(testImage|label)*P(label)\n",
    "\\end{aligned}\n",
    "$$\n",
    "**and print out each label's probability given a particular testImage where** \n",
    "\n",
    "$\\text{pbgt}=\\text{particular (bin) of gray level from testImage}$\n",
    "\n",
    "$\\text{prior}=P(label)=\\frac{\\text{amount of training images of the label appear}}{\\text{total amount of training image}}\\text{ , the percentage of each label in the training dataset}$\n",
    "\n",
    "$\\text{particular image's likelihood}=P(testImage|label)\\\\=P(1^{st} pixel=pbgt , 2^{nd} pixel=pbgt , ... , and (28*28)^{th} pixel=pbgt|label\\text{ , each pixel's gray level is between 0 and 255)}  \\\\\\text{ps. there are }256^{784} \\text{possible images}$\n",
    "\n",
    "**assume all the features (28*28 pixels) is independent in the condition of that label (the essence of naive bayes classifier)**\n",
    "\n",
    "$\\approx P(1^{st} pixel=pbgt|label) * P(2^{nd} pixel=pbgt|label) * P(3^{rd} pixel=pbgt|label) * ... * P((28*28)^{th} pixel=pbgt|label)$\n",
    "\n",
    "**in discrete mode:**\n",
    "\n",
    "we parse each pixel's gray level (0-255) into 32 bins (0-7,8-15,...)\n",
    "\n",
    "$\\approx P(1^{st} pixel=pbgt , 2^{nd} pixel=pbgt , ... , and (28*28)^{th} pixel=pbgt|label\\text{ , each pixel's bin of gray level is between 0 and 31)}  \\\\\\text{ps. there are }32^{784} \\text{possible images}$\n",
    "\n",
    "$\\text{particular pixel's likelihood} = P(n^{th} pixel=pbgt|label) = \\frac{\\text{count of } n^{th} \\text{ pixel=(pbgt) under training image's particular label}}{\\text{sum of }n^{th} \\text{ pixel's count of each bin of gray level under training image's particular label}}$\n",
    "\n",
    "**in continuous mode:**\n",
    "\n",
    "in this case, every pixel has its own Gaussian distribution with ($\\mu$=mean of each value of the pixel under a particular label, $\\sigma^2$ variance of each value of the pixel under a particular label)\n",
    "\n",
    "$\\text{particular pixel's likelihood}=P(n^{th} pixel=pbgt|label)=\\frac{1}{\\sigma \\sqrt{2\\pi}}exp({-\\frac{(n^{th}pixel-\\mu)^2}{2\\sigma^2})}$\n",
    "\n",
    "the reason why we don't need to multiply probability density function of Gaussian(height) by $\\Delta x$(width) to reach the definition of probability is because we just interest in the relative likelihood that a pixel belongs to each labels\n",
    "$$\n",
    "P(label|testImage)=\\frac{(P(testImage|label)*\\Delta x)*P(label)}{\\sum_{label}(P(testImage|label)*\\Delta x)*P(label)}=\\frac{P(testImage|label)*P(label)}{\\sum_{label}P(testImage|label)*P(label)}\n",
    "$$\n",
    "**because log is monotonic, we can add log scale on the posterior to avoid underflow**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{log posterior}&=log P(label|testImage) \\\\\n",
    "&=log(\\frac{P(testImage|label)*P(label)}{P(testImage)}) \\\\\n",
    "&\\because \\text{log posterior's denominator }P(testImage)\\text{ is irrelevant to each label's probability} \\\\\n",
    "&\\therefore \\text{we can just use log posterior's numerator instead of whole log posterior} \\\\ \n",
    "&\\propto log(P(testImage|label)*P(label))\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "**we use log posterior's numerator to represents each label's probability, but the sum of each log posterior's numerator isn't equal to 1 which is opposed to the probability's definition**\n",
    "\n",
    "**we hope the output of sum of each label's probability is equal to 1, so we multiply each log posterior's numerator by $\\alpha(\\alpha<0)$ to achieve normalization**\n",
    "$$\n",
    "\\begin{align}\n",
    "1&=\\alpha \\sum_{label}log(P(testImage|label)*P(label)) \\\\\n",
    "\\therefore \\alpha &=\\frac{1}{\\sum_{label}log[P(testImage|label)*P(label)]}\n",
    "\\end{align}\n",
    "$$\n",
    "**which means that each label's probability under the particular testImage is equivalent to multiply each log posterior's numerator by $\\alpha$**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{normalized log posterior} &=\\alpha*log[P(testImage|label)*P(label)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "**we can calculate the posterior based on each pixel's likelihood instead of an image's likelihood,**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&=\\alpha*\\{log[(P(1^{st} pixel=pbgt|label) * P(2^{nd} pixel=pbgt|label) * ... * P((28*28)^{th} pixel=pbgt|label)) * P(label)]\\} \\\\\n",
    "&=\\alpha*\\{logP(1^{st} pixel=pbgt|label) + logP(2^{nd} pixel=pbgt|label) + ... + logP((28*28)^{th} pixel=pbgt|label) + logP(label)\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "**based on the above statement, the most potential lable can be derived in this way**\n",
    "$$\n",
    "\\begin{aligned}\n",
    "label^* &=argmax_{label}P(testImage|label)*P(label) \\because \\text{log is a monotonic function}\\\\\n",
    "&=argmax_{label}log[P(testImage|label)*P(label)] \\because \\alpha<0\\\\\n",
    "&=argmin_{label}\\alpha*log[P(testImage|label)*P(label)]\\\\\n",
    "&=argmin_{label} \\text{ normalized log posterior} \\\\\n",
    "&\\therefore \\text{normalized log posterior's corresponding order of each label's probability is reverse}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Imagination(likePixelProb,modelParameter):\n",
    "    fout=open('./dataset/out_Imagination.txt',mode='w')#sys.stdout#\n",
    "    #Imagination of Naive Bayes Classifier\n",
    "    #after getting each pixel's each bin of gray level's likelihood, we can get the imagination of numbers in our naive Bayes classifier\n",
    "    #each pixel's bin of gray level is the one that makes this pixel's bin of gray level's likelihood maximum\n",
    "    (nRow,nCol,nLabel,_,_,_,_)=modelParameter\n",
    "    nBins=likePixelProb.shape[3] #discrete=32,continuous=256\n",
    "    for label in range(nLabel): #deal with each number imagination\n",
    "        for x in range(nRow):\n",
    "            for y in range(nCol):\n",
    "                vBin=np.argmax(likePixelProb[label][x][y][:])\n",
    "                if vBin>=(nBins/2):\n",
    "                    print(\"1\",end=\" \",file=fout)\n",
    "                else:\n",
    "                    print(\"0\",end=\" \",file=fout)\n",
    "            print(\" \",file=fout)\n",
    "        print(\"\\n\",file=fout)\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prior(trainLb,modelParameter):\n",
    "    (_,_,nLabel,_,_,_,_)=modelParameter\n",
    "    #prior : the percentage of each label in the training dataset\n",
    "    priorProb=np.zeros(nLabel)\n",
    "    for label in range(nLabel):\n",
    "        priorProb[label]=np.sum(trainLb==label)/len(trainLb)\n",
    "    return priorProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LikePixel_Discrete(trainImg,trainLb,modelParameter):\n",
    "    (nRow,nCol,nLabel,nBins,nPixelValue,_,_)=modelParameter\n",
    "    divBin=int(nPixelValue/nBins)\n",
    "    #particular pixel's likelihood : calculate the likelihood of each pixel's bin of gray level in the training dataset\n",
    "    likePixelProb=np.zeros((nLabel,nRow,nCol,nBins))\n",
    "    #nominator , classify all the image into each of label, and count the amount of each pixel's bin of gray level under the particular label\n",
    "    for i in tqdm(range(len(trainImg))): #each training image\n",
    "        label=trainLb[i] #attach to the particular label\n",
    "        for x in range(nRow): #each pixel\n",
    "            for y in range(nCol):\n",
    "                locBin=trainImg[i][x][y]//divBin #parse each pixel's value into 32 bins\n",
    "                likePixelProb[label][x][y][locBin]+=1 #count the amount of this pixel's bin of gray level\n",
    "    #denominator , count all the possible bins of gray level of each label's each pixel\n",
    "    for label in range(nLabel): #each label\n",
    "        for x in range(nRow): #each pixel\n",
    "            for y in range(nCol):\n",
    "                sumBin=0\n",
    "                for iBin in range(nBins): #sum up all this pixel's possible bin of gray level\n",
    "                    sumBin+=likePixelProb[label][x][y][iBin]\n",
    "                likePixelProb[label][x][y][:]/=sumBin\n",
    "    return likePixelProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x,mean,variance):\n",
    "    return ((1/math.sqrt(2*math.pi*variance))*math.exp((-(x-mean)**2)/(2*variance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LikePixel_Continuous(trainImg,trainLb,modelParameter):\n",
    "    (nRow,nCol,nLabel,_,nPixelValue,minVar,_)=modelParameter\n",
    "    likePixelProb=np.zeros((nLabel,nRow,nCol,nPixelValue))\n",
    "    #debug1=np.zeros((nLabel,nRow,nCol))\n",
    "    for label in tqdm(range(nLabel)):\n",
    "        for x in range(nRow):\n",
    "            for y in range(nCol):\n",
    "                mean=np.mean(trainImg[label==trainLb][:,x,y]) # you can't write trainImg[label==trainLb][:][x][y], because X[:] is just to cut the sample space(eg. 3x3 -> 2*3), not dereference. therefore, X[:][0:3]=X[0:3], X[:][0]!= X[:,0]\n",
    "                variance=np.var(trainImg[label==trainLb][:,x,y])+minVar # to avoid gaussian divide by zero, variance should be zero, but it can't be to small, because it will be put on the exponential term, or it will be e^-(1/(0.001=1e-3))=e^-(1e+3) which leads to underflow and can not calculate likelihood efficently\n",
    "                #debug1[label][x][y]=variance\n",
    "                for p in range(nPixelValue):\n",
    "                    likePixelProb[label][x][y][p]=gaussian(p,mean,variance) #because this Guassian distribution so sharp that all the probability is equal to mean; the values of probability density function can be greater than one. the actual probability P(X<x) should be an intergral. https://stats.stackexchange.com/questions/9427/interpreting-gaussian-probabilities-greater-than-1\n",
    "    #debug2=np.reshape(debug1,(nLabel*nRow*nCol)) #check debug2[(debug2<1) & (debug2>0)]\n",
    "    return likePixelProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayesClassifier(trainImg,trainLb,testImg,testLb,modelParameter,toggle):\n",
    "    #Naive Bayes Classifier in discrete mode\n",
    "    fout=open('./dataset/out_NaiveBayesClassifier.txt',mode='w')#sys.stdout#\n",
    "    (nRow,nCol,nLabel,_,_,_,minNum)=modelParameter\n",
    "    \n",
    "    #prior\n",
    "    priorProb=Prior(trainLb,modelParameter)\n",
    "    #each of particular pixel's likelihood\n",
    "    if toggle==0:\n",
    "        likePixelProb=LikePixel_Discrete(trainImg,trainLb,modelParameter)\n",
    "    else:\n",
    "        likePixelProb=LikePixel_Continuous(trainImg,trainLb,modelParameter)\n",
    "    #normalized log posterior\n",
    "    error=0\n",
    "    for i in tqdm(range(len(testImg))): #each testing image\n",
    "        posteriorLogProbNumerator=np.zeros(nLabel)\n",
    "        for label in range(nLabel): #calcalate each lable's posterior's numerator\n",
    "            #each of particular image's likelihood\n",
    "            likeImageLogProb=0\n",
    "            for x in range(nRow):\n",
    "                for y in range(nCol):\n",
    "                    if(toggle==0):\n",
    "                        divBin=int(nPixelValue/nBins)#256(=1byte) is the size of each pixel which is defined in the dataset\n",
    "                        locBin=testImg[i][x][y]//divBin #see which bin the testing image's each pixel is locate on\n",
    "                        likeImageLogProb+=np.log(max(minNum,likePixelProb[label][x][y][locBin])) #calculate the log probability of likelihood by adding each log probability of pixel's likelihood; the reason why we can directly add the log probability is because of conditionaly independence; why we need lower bound? the domain of a logarithmic function is larger than zero, so each bin's minimum value is larger than zero\n",
    "                    else:\n",
    "                        locPixel=testImg[i][x][y]\n",
    "                        likeImageLogProb+=np.log(max(minNum,likePixelProb[label][x][y][locPixel]))#minNum can't be too small, because the likePixelProb derived from Gaussian can be small to 1e-300. also, python's underflow is 1e-3xx\n",
    "            #log posterior's numerator = log(particular image's likelihood * prior) = log(particular image's likelihood) * log(prior)\n",
    "            posteriorLogProbNumerator[label]=likeImageLogProb+np.log(priorProb[label])\n",
    "        #normalize the log posterior : because it is divided by the sum of posterior's numerator which is negative, the probability of each label is the reverse order of normalized log posterior\n",
    "        normPosteriorLogProb=posteriorLogProbNumerator/np.sum(posteriorLogProbNumerator)\n",
    "        #pick the most potential label under this given testImage which is the minimal normalized log posterior\n",
    "        predict=np.argmin(normPosteriorLogProb)\n",
    "        #caculate total error\n",
    "        error+=(predict!=testLb[i])\n",
    "        \n",
    "        #print each label's probability in log scale\n",
    "        for label in range(nLabel):\n",
    "            print(label,\": \",normPosteriorLogProb[label],file=fout)\n",
    "        print(\"Prediction: \",predict, \" Ans: \", testLb[i],end=\"\\n\\n\",file=fout)\n",
    "    #print this classifier's error rate\n",
    "    errorRate=error/len(testImg)\n",
    "    print(\"Error rate: \",errorRate,file=fout)\n",
    "    fout.close()\n",
    "    #print imaignation of each label under this classifier\n",
    "    Imagination(likePixelProb,modelParameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [01:30<00:00, 662.30it/s]\n",
      "100%|██████████| 10000/10000 [00:15<00:00, 645.89it/s]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Toggle option (0:discrete / 1:continuous):  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [05:56<00:00, 168.32it/s]\n",
      "100%|██████████| 10000/10000 [11:48<00:00, 14.10it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    #initialize\n",
    "    ((trainImg,trainLb),(testImg,testLb),nRow,nCol)=Readfile()\n",
    "    nLabel=10\n",
    "    nPixelValue=256\n",
    "    nBins=32\n",
    "    minVar=10#1e-1#100 or 1000 is better\n",
    "    minNum=1e-30#1e-10\n",
    "    modelParameter=(nRow,nCol,nLabel,nBins,nPixelValue,minVar,minNum)\n",
    "    \n",
    "    #implement Naive Bayes Classifier\n",
    "    toggle=int(input('Toggle option (0:discrete / 1:continuous): '))\n",
    "    NaiveBayesClassifier(trainImg,trainLb,testImg,testLb,modelParameter,toggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "def normal_distribution(x, mean, sigma):\n",
    "    return np.exp(-1*((x-mean)**2)/(2*(sigma**2)))/(math.sqrt(2*np.pi) * sigma)\n",
    "\n",
    "\n",
    "mean1, sigma1 = 0, 0.1\n",
    "x1 = np.linspace(mean1 - 6*sigma1, mean1 + 6*sigma1, 100)\n",
    "\n",
    "mean2, sigma2 = 1, 0.01\n",
    "x2 = np.linspace(mean2 - 6*sigma2, mean2 + 6*sigma2, 100)\n",
    "\n",
    "mean3, sigma3 = 2, 1e-6\n",
    "x3 = np.linspace(mean3 - 6*sigma3, mean3 + 6*sigma3, 100)\n",
    "\n",
    "y1 = normal_distribution(x1, mean1, sigma1)\n",
    "y2 = normal_distribution(x2, mean2, sigma2)\n",
    "y3 = normal_distribution(x3, mean3, sigma3)\n",
    "\n",
    "plt.plot(x1, y1, 'r', label='m=0,sig=0')\n",
    "plt.plot(x2, y2, 'g', label='m=0,sig=0.1')\n",
    "plt.plot(x3, y3, 'b', label='m=0,sig=0.01')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
